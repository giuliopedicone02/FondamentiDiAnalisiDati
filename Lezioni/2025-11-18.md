# Martedì 18 novembre 2025 - Classificatore Logistico Multiclasse (+ recupero Regressione Logistica)

# Recupero Regressione Logistica

L'odd è pari a 1 quando abbiamo equi-probabilità

$$
Odd = \frac{p}{1-p} = \frac{0.5}{1 - 0.5} = 1
$$

## Esempio di regressore logistico semplice applicato sul dataset Breast Cancer

Il regressore ha un problema, `compactness` ha un p-value alto, facciamo backward elimination la togliamo e a questo punto il nostro regressore va bene. Possiamo interpretarlo:

L'intercetta è -2.67, lo esponenziamo ed ottengo circa 0.07 (7:100 come odd), se tutte le variabili fossero uguali a 0 ci sono su 107 tentativi 7 volte sono in presenza della classe 1 e le restanti 100 volte sono in presenza della classe 0, è molto probabile cvlassificare come classe 0.

Prendiamo il coefficiente di texture1 che è 0.2219 lo esponenzio ed ottengo 1.02, significa che sto sommando il 2% del suo valore, diventa più probabile la classe Y=1 quando tutte le altre variabili sono costanti

Prendiamo il coefficiente di perimeter1 che è -0.4419, esponenziato è 0.96, significa che ho un -4% di probabilità di ottenere la classe Y=1 quando tutte le altre variabili sono costanti

## Valutiamo il modello di regressione logistica

Non possiamo più utilizzare l'R2 utilizzato nella regressione lineare, nel caso della regressione logistica non ho le probabilità di ground-truth. Nel nostro caso definiamo R2 come:

$$
R2 = 1 - \frac{J(\beta)}{J(\beta)}
$$

Dove il caso peggiore è un classificatore randomico.

## R2 di McFadden

Definiamo l'R2 di McFadden come


$$
R^2_{McFadden} = 1 - \frac{LL_{Model}}{LL_{Null}}
$$

Dove $LL_Null$ è il mio modello "pessimo" di base che mi predice sempre 0.5.

Un valore di R2 di McFadden di 0.77 indica che il modello spiega il 77% dell'incertezza dell'outcome ma resta il 23% che il modello non riesce a spiegare. 

## Metriche di valutazione predittive

Sono:
- Accuracy
- Precision
- Recall
- F1 Score
- Confusion Matrix

---

# Classificatore Logistico Multiclasse

Vogliamo estendere il regressore logistico al caso multiclasse. Il classificatore può funzionare su un numero arbitrario di classi. Utilizziamo il dataset relativo al diabete e raggruppiamo per 3 classi: `Normal`, `Chemical Diabetic` ed `Overt Diabetic`.

Abbiamo due modi per estendere il regressore logistico, sia dal punto di vista statistico che quello machine learning

## Regressione Logistica Multinomiale

Se un regressore logistico ci dava l'odd che un evento si verifichi fratto la probabilità che un evento non si verifichi, in questo caso voglio stimare diverse frazioni di probabilità. Mi chiedo "Quanto è probabile osservare la classe 1 contro la classe 2? Quanto è probabile osservare la classe 1 contro la classe 3?" Posso vedere quale di queste domande ha come risposta la probabilità maggiore.

Scelgo una classe e la chiamo classe di base, nel mio caso potrebbe essere `normal`, faccio quindi due confronti:

- `normal` vs `chemical diabetic`
- `normal` vs `overt diabetic`

Seguo una distribuzione multinomiale e vado a modellare il logaritmo della probabilità di osservare una data classe k fratto la probabilità di osservare la classe base (sepsso indicata come 1). Le probabilità sono quindi: "Avviene A / Avviene qualcosa di diverso da A".

Mi restano k-1 classi una è la classe di base, ho bisogno di k-1 rapporti e quindi ho bisogno di k-1 regressori. Ora ho $(n+1)(k-1)$ parametri.

A partire da questa formulazione per le classi non-baseline ho:

$$ 
P(Y=k|X=\mathbf{x}) = \frac{e^{\mathbf{\beta_k}^T\mathbf{X}}}{1+\sum_{l=2}^K \mathbf{e^{\beta_l^T\mathbf{X}}}}
$$

Per la classe baseline ho:

$$ P(Y=1|X=\mathbf{x}) = \frac{1}{1+\sum_{l=2}^K \mathbf{e^{\beta_l^T\mathbf{X}}}}$$

## Esempio

Si prende una classe di base e fitta 2 diversi regressori logistici, la tabella è divisa in due parti: group = 1 e group = 2 con group = 0 come classe di default. Per interpretarla devo prendere tutti i miei coefficienti ed esponenziarli. 

Prendo l'intercetta del gruppo 1 è -90.37 esponenziato è circa 0, ci dice che se glutest è uguale a 0 molto probabilmente siamo nel caso della classe base in modo infinitsamente più probabile. 

Anche nell'altro caso ho come intercetta del gruppo 2 -109.82 esponenziato è sempre circa 0, caso analogo al precedente.

Prendendo l'intercetta del gruppo 1 di glutest, all'incremento di una unità diventa il 24% più probabile di essere un paziente con diabete di tipo chemical.

Prendendo l'intercetta del gruppo 2 di glutest, all'incremento di una unità diventa il 28% più probabile di essere un paziente con diabete di tipo overt.

Si arriva prima ad Overt che Chemical in pratica. Il regressore è utilizzato in termini statistici solo quando ho una classe di base chiara, ma non sempre è possibile farlo, sul diabete ha senso, sul dataset Iris è difficile scegliere una classe di base.

Vediamo ora algoritmi che si discostano dalla statistica favorendo un approccio più complesso ma meno interpretabile.

## Interpretazione Geometrica dei coefficienti di un regressore logistico

Sappiamo che

$$P\left( y=1 | \mathbf{x} \right) = \frac{1}{1 + e^{- ({\beta}_{0} + \beta_{1}x_{1} + \beta_{2}x_{2})}}$$

In forma esplicita il deciion boundary è dato da:

$$P\left(y=1 | \mathbf{x} \right) = 0.5 \Leftrightarrow e^{- (\beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2})} = 1 \Leftrightarrow 0 = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2}$$

Ottenendo:

$$x_{2} = - \frac{\beta_{1}}{\beta_{2}}x_{1} - \frac{\beta_{0}}{\beta_{2}}$$

Ottengo quindi una retta che separa le due classi.

> SVM mantiene un bias analogo ma abbassa la varianza.

## Softmax

Ipotizzo che per me le classi siano tutte uguali, faccio una overparametrizzazione ed introduco la **softmax**:

$$ P(Y=k|X=\mathbf{x}) = \frac{e^{\mathbf{\beta_k}^T\mathbf{X}}}{\sum_{l=1}^K \mathbf{e^{\beta_l^T\mathbf{X}}}}, \ \ \ \forall k=1,\ldots,K$$

Mi permette di fare il fit di k logit che vanno da [-inf, +inf], espoenzio per tornare nel mondo degli odd e divido per la sommatoria di tutti i logit per tornare nel range [0,1]. Ottengo quindi un **classificatore multiclasse**. Anche in questo caso ho un modello lineare, i decision boundary li ottengo quando le probabilità tra la classe i e la classe j sono uguali. Ma quando le probabilità sono uguali?
$$
P(Y=i|\mathbf{x}) = P(Y=j|\mathbf{x})
$$

Equivalente a dire:

$$
\beta_i^T \mathbf{x} = \beta_j^T \mathbf{x}
$$

Questa equazione definisce un iperpiano che divide lo spazio in due regioni.

## Da classificatore binario a multi-classe: One vs All (One vs Rest)

Ci permette di trasformare qualsiasi classificatore binario in un classificatore multiclasse. Immaginiamo di avere 3 classi, utilizziamo 3 classificatori One vs Rest rietichetta le classi in 3 modi diversi. Ottengo uno score per ogni classificatore per classificare la nuova istanza faccio un argmax tra tutti i miei classificatori e sceglierò la classe che mi darà lo score maggiore.

Esiste anche la tecnica One vs One applicata anche dal regressore softmax.