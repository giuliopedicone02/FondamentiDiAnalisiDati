# Lunedì 27 ottobre 2025 - Associazioni tra variabili

## Da analisi univariata ad analisi multivariata

L'analisi univariata esamina le variabili individualmente una per volta. Esempio quello di calcolare la media ottenuta in un test

L'analisi multivariata studia le interazioni tra variabili multiple come ad esempio capire come cambia la media di un test rapportata alle ore di studio

Due variabili si dicono associate se osservare il valore di una di queste due assume dice qualcoda sul valore che l'altra variabile assumerà, si può definire anche come **correlazione**.

## Cos'è l'associazione

Ha a che fare con il concedtto di dipendenza in probabilità, alcuni esempi di variabili associate sono peso ed altezza, ci aspettiamo che le persone più alte pesino sdi più, altro esempio è l'età e la pressione sanguigna una persona più anziana tende ad avere valori di pressione più alti

## Correlazione diversa dal rapporto di causa-effetto

Supponiamo di avere due variabili:
- numero di gelati che vendo in un mese
- numero di incidenti (persone che annegano) in un mese

Mi aspetto che tracciando questi valori in diversi mesi, mi aspetto un aumento di entrambe le variabili nei mesi più caldi, vedendo questi numeri **sembra** che queste due variabili siano associate, ma è chiaro che non ci sia un rapporto di causa-effetto.

Posso avere un rapporto di causa-effetto tra l'aumento della temperatura e le persone che nuotano e quindi un rapporto di causa-effetto con le persone che annegano.

## Esempio applicato sul dataset Titanic

Possiamo porci domande come:
- la classe del passeggero influenza la probabilità di sopravvivenza?
- l'età influenza la probabilità di sopravvivenza?
- la tariffa pagata influenza la probabilità di sopravvivenza?

## Misure di associazione per le variabili discrete

Gli strumenti che funzionano per le variabili discrete generalmente non sono i migliori per le variabili continue e viceversa. Nel nostro dataset le variabili discrete che possiamo considerare sono: **Survived** e **Pclass**

Posso fissare la classe e calcolare la probabilità di sopravvivenza per le varie classi. Ci chiediamo se esiste una correlazione tra queste due variabili. Dal grafico si nota in effetti che i sopravvisutti della prima classe sono di più rispetto alla terza classe, sembra esserci quindi una correlazione.

## Come misuriamo la correlazione

Utilizziamo una tabella di contingenza, mettiamo due variabili uno nell'asse x uno nell'asse y gli incroci indicano il numero di volte in cui quei due valori sono uguali, aggiungo inoltre una riga ed una colonna alla fine che somma i valori presenti nelle righe e nelel colonne.

I valori marginali mi permettono di costruire le probabilità marginali, l'ultima colonna in particolare mi definisce P(X) e l'ultima riga la probabilità P(Y). Tutti i restanti valori ci danno le probabilità congiunte $ P(X=x_i, Y=y_j) $

## Indipendenza e Frequenze

Se le variabili sono indipendenti posso costruire la congiunta delle marginali, a questo punto potrei prendere la tabella di contingenza scartare i valori al suo interno tranne la sua ultima riga e la sua ultima colonna e ricostruire i valori al suo interno.

Ricostruendola ci sono grandi differenze tra queste due tabelle, questo ci dice che in realtà non c'è molta correlazione tra queste due variabili, altrimenti le due tabelle avrebbero avuto valori molto simili tra loro.

$$
P(X=x_i,Y=y_j) = P(X=x_i)P(Y=y_i)
$$

Il numero di volte in cui $P(X = X_i)$ è uguale ad $\frac{n_{i+}}{n}$ allo stesso modo $P(Y=y_j) = \frac{n_{+j}}{n}$

## Statistica di Pearson's

Ha due sommatorie una per le righe ed una per le colonne e sottraggo i valori veri da quelli ricostruiti, elevo al quadrato per ottenere sempre un valore positivo e divido per n stimato per tornare sulla scala di misura corretta.

Se ho divergenza bassa le due variabili erano indipendenti se ho una sivergenza alta le due variabili sono dipendenti, quindi un valore alto indica correlazione un valore basso indica mancanza di correlazione.

Non è generalmente utilizzata questa misura perchè ha un problema, il valore più basso che posso ottenere è 0 che indica che le due variabili sono effettivamente indipendenti, 0 è difficole da ottenere ma valori prossimi allo zero indicano effettivamente indipendenza. Il problema è che non esiste un valore massimo, non abbiamo un upper-bound e questi valori sono difficili da interpretare.

> Se nella matrice di contingenza ho valori inferiori a 5 questa statistica non è molto affidabile

## Statistica di Cramer

Versione normalizzata tra 0 ed 1 della statistica di Pearson's dove:
- 0 indica mancanza di correlazione
- 1 indica correlazione perfetta o sono identiche o una è ottenuta moltipicando la prima per una costante

0.34% indica una associazione debole.

## Misure di associazione tra Variabili continue

Prendiamo due variabili continue a caso dal dataset sul diabete e utilizziamo uno strumento chiamato **scatterplot** che permette di visualizzare le osservazioni come dati bi-dimensionali.

Due variabili si dicono:
- correlate positivamente se a valori alti di x corrispondono valori alti di y
- correlate negativamente se a valori bassi di x corrispondono valori bassi di y
- non correlate se i puntini nello scatterplot sono sparsi

## Scatter Matrix

Prende solo le variabili continue e scarta quelle discrete e fornisce tutti i possibili scatterplot per le variabili continue, sulla diagonale principale non troviamo scatterplot perchè non sarebbero molto informativi ma viene visualizzato un istogramma non normalizzato delle frequenze suddiviso in bins trattandosi di variabili continue. La matrice è simmetrica.

Permette di capire quali sono le variabili più correlate tra loro, facendoci un'idea tra quelle correlate linearmente e quelle correlate non linearmente.

## Hexbin Plot

Visualizzazione analoga allo scatter plot. Istogramma bi-dimensionale dove divido in celle sia l'asse x che l'asse delle y, per poi contare quanti puntini ricadono in ogni cella, i colori con intensità più bassa indicano meno densità i colori con intensità più alta indicano densità più elevata.

## Stima di densità

Permette di capire dove sono concentrati maggiormente gli elementi, a questo grafico è associato solitamente un grafico sulle curve di livello, tutto ciò che sta sulla stessa curva di livello è relativo alla stessa densità.

## Covarianza

Modo per misurare un'associazione lineare è un concetto molto vicino a quello della varianza, posso cercare di capire quanto le distanze tra la media di X e la media di Y sono simili. Calcolo la distanza di un punto rispetto alle due medie:

- Se entrambe le distanze sono positive indica una correlazione positiva (x ha valori grandi, y ha valori grandi)
- Se entrambe le distanze sono negative indica una correlazione negativa (x ha valori bassi, y ha valori bassi)

Negli altri due quadranti abbiamo situazioni discorde.

La formula della covarianza è simile a quello della varianza ma faccio lo stesso calcolo sia per x che per y.

Se la covarianza ha un valore positivo indica correlazione positiva, se la covarianza è negativa indica una correlazione negativa, una covarianza prossima allo zero indica assenza di correlazione. 

Lo stimatore non distorto divide il tutto per n-1.

La covarianza va bene in caso di correlazione lineare ma ci sono casi come quello della parabola o del cerchio che ha una correlazione non lineare forte ma che la covarianza non rileva restituendo valori prossimi allo zero.

Serve una misura normalizzata, la covarianza ha problemi di unità di misura perchè peso ed altezza hanno unità di misura differenti, se porto i kg in grammi e i metri in millimetri mi aspetto che la covarianza cresca.

Utilizziamo la tecnica dello **z-scoring** che elimina l'unità di misura rendendo le due variabili indipendenti dalla loro unità di misura. Qesto valore prende il nome di **indice di correlazione di Pearson**

## Indice di correlazione di Pearson

Rappresenta la covarianza divisa per le deviazioni standard di x e di y. Cambiano gli assi del grafico poichè compreso tra -1 ed 1. Diventa quindi facilmente interpretabile:

- -1: Indica la massima anti-correlazione possibile
- 1: Indica la massima correlazione positiva possibile

Tra 0 e 0.3 indica una correlazione debole, tra 0.3 e 0.7 indica una correlazione moderata tra 0.7 e 1.0 indica una correlazione forte.

Limitazione: va bene solo per correlazioni lineari.


## Matrice di covarianza

La Covarianza(X,X) è uguale alla varianza di X quindi $Cov(X,X)=Var(X)$

Nella diagonale principale troviamo solo varianze, la matrice è simmetrica essendo il prodotto commutativo. Guardando il grafico lo "spessore" tra i punti se maggiore indica una correlazione sempre più distante (piccola) da 1, maggiore spessore = correlazione più bassa.

Solo il segno della correlazione di pearson ci dice qualcosa sull'orientamento della retta ma la magnitudo non ci fornisce ulteriori informazioni.

## Quando utilizzare l'indice di correlazione di Pearson

Lo posso utilizzare quando le variabili sono continue, non è consigliabile usarlo in caso di variabili nominali e nemmeno nei casi in cui sono ordinali è una buona idea. Esiste una eccezione la point-biserial correlation se una delle due è continua e l'altra è binaria funziona correttamente.

Ad esempio se volessimo calcolare la correlazione tra sesso (binaria) ed età (continuo) allora possiamo utilizzare la correlazione 

## Indice di correlazione di Spearman

Cattura anche i casi di correlazione non lineare, supponiamo di avere 4 partecipanti per capire quale gelato è il preferito tra i bambini ogni bambino dà uno score ad ogno gusto di gelato, vogliamo cercare di capire i gusti di gelato più gettonato.

Per ogni partecipante si ordinano i gusti per ranking, si crea una nuova tabella con i rankingdei vari gusti per ogni partecipante, Spearman cerca di capire se esiste una correlazione tra i vari valutatori ed i ranking ottenuti. In questo caso Pearson potrebbe non trovare una buona correlazione ma Spearman trova una altissima correlazione.

## Indice di correlazione di Kendall

Cerca di risolvere lo stesso problema, quello dei giudici che utilizzano scale di valutazione differenti ma lo fa in modo computazionalmente più oneroso. Ogni mio elemento lo posso vedere come punto in un grafo cortesiano, posso prendere quindi una coppia di punti nel grafico e capire se sono concordi o discordi, quindi se entrambi i giudici li hanno ordinati nella stessa maniera o meno.

Ottengo quindi una percentuale di coppue concorde e discorde, posso capire quindi se i due giudici sono correlati o meno nella valutazione. Un valore di 0.7 indica che i due giudici hanno valutato per il 70% alla stessa maniera i due gelati.

## Matrici di correlazione e Heatmap

Grafico che tramite scala di colore fornisce un indice di correlazione tra tutte le variabili del nostro dataset, nella diagonale principale otteniamo sempre 1 tutti i valori sono normalizzati tra -1 ed 1. Di default il metodo è Pearson ma possiamo specificare anche Spearman e Kendall.