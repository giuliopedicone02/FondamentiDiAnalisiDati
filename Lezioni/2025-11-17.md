# Lunedì 17 novembre 2025 - Regressione Logistica

A differenza di K-NN è utile ottenere un modello parametrico di classificazione, il k-nn ha limitazioni quando abbiamo molte features, richiede la memorizzazione dell'intero training set e la classificazione è molto onerosa in termini computazionali. Un modello parametrico è veloce, compatto e le predizioni sono abbastanza naturali.

## Limiti della regressione lineare

Potrei cercare di costruire un regressore lineare che mi permetta di effettuare classificazione binaria, ma è un modello buono? Verifichiamolo plottando i residui, la nostra funzione è sbagliata in quanto mappa da valori reali a valori realli ma noi vogliamo mappare in {0,1}. Il modello è inoltre difficile da utilizzare, produce valori fuori dal range [0,1]. Inoltre abbiamo bisogno dell'assunzione di linearità dai dati.

Possiamo pensare che invece di predire le classi {0,1} vorrei cercare di stimare una probabilità che sappiamo essere compresa in [0,1].

Sappiamo che P(y=0) = 1 - P(y=1). Ci aspettiamo una funzione continua, ma è anche lineare? No

Vogliamo ottenere una versione continua di una funzione a gradino che funzioni come una soglia e che mi permetta di effettuare classificazione. Vogliamo trovare una formulazione analitica di questa funzione e capire dove mettere il centro di questa funzione e quanto la mia funzione deve essere morbida o rigida.

## Dai valori binari alle probabilità

I modelli discriminativi come il K-NN calcola una probabilità condizionale guardando la frequenza relativa. Vogliamo trovare una trasformazione della regressione lineare che mi rende il tutto più gestibile, utilizziamo uno strumento detto **odd**.

## Cosa sono gli odd

Supponiamo di avere un evento binario, definisco un odd come la probabilità che l'evento accada diviso la probabilità che un evento non accada. Funzione che permette di passare dal range [0,1] al range [0,+inf] questo risolve uno dei problemi.

L'odd viene spesso indicato come una frazione del tipo (3:1). Immaginiamo di voler scommettere sulla vincita tra due squadre A e B, il bookmarker stima la vincita e dice che A ha un vantaggio rispetto a B. Significa che su 4 partite ci si aspetta che il team A vinca 3 partite ed una partita la vinca B. Se scommetto sul team che reputo perdente il bookmaker mi fornirebbe fino a 3 volte tanto l'importo della scommessa.

Posso calcolare la probabilità di vincita come:

$$
P = \frac{Odd}{Odd+1}
$$

Nel nostro caso la probabilità di vincita del team A è di 0.75 (L'odd è 3 a 1).

Se prendessi il logaritmo dell'odd (definito tra [0, +inf]) esce fuori una funzione definita tra [-inf, +inf] e prende il nome di  **log-odd**. Da qui nasce la funzione logistica.

## La funzione logistica

La funzione logistica (o sigmoide) è definita come:

$$f(x) = \frac{1}{1+e^{-x}}$$


Ha un range [0,1], satura ad 1 ed è differenziabile.

Posso creare un regressore logistico sostituendo ad x la formula della regressione.

## Regressione Logistica

$$P(y=1|\mathbf{x}) = \frac{1}{1+e^{-(\beta_0 + \beta_1 x_1 + \ldots + \beta_n x_n)}}$$

Nel caso di regressione lineare semplice, in che modo i coefficienti $\beta_0 e \beta_1$ influenzano la retta? Sono i parametri che mi dicono come la nostra sigmoide è orientata e quanto essa sia "stretchata".

Il numero di nepero elevato alla combinazione delle features ci restituisce l'odd visto in precedenza. Il logaritmo dell'odd (log-odd) viene anche detto **logit** è lineare ed è nel range [-inf, +inf]

## Come trovo i parametri del regressore logistico

Non posso più utilizzare RSS, il regressore logistico non ce lo permette. Se volessi applicare l'MSE qui come posso fare? Facciamo una stima di **massima verosimiglianza**, ci chiediamo qual è la probabilità dei dati, dato quel sigmoide? Come facciamo a capire qual è il sigmoide che fitta meglio i nostri dati?

Idealmente vogliamo che la probabilità associata ad elementi positivi sia alta mentre quella associata ad elementi negativi sia bassa.

$$P\left( y \middle| \mathbf{x};\mathbf{\beta} \right) = \left( f_{\mathbf{\beta}}\left( \mathbf{x} \right) \right)^{y}\left( 1 - f_{\mathbf{\beta}}\left( \mathbf{x} \right) \right)^{1 - y}$$

Possiamo immaginarla come una funzione definita per casi: 

- $$P\left( y = 1 \middle| \mathbf{x};\mathbf{\beta} \right) = f_{\mathbf{\beta}}(\mathbf{x})$$
- $$P\left( y = 0 \middle| \mathbf{x};\mathbf{\beta} \right) = 1 - f_{\mathbf{\beta}}(\mathbf{x})$$

Dobbiamo assumere che i dati siano tutti indipendenti e che siano identicamente distribuiti ovvero che seguano la stessa distribuzione, questo ci permette di dire che la mia probabilità del mio dataset che essendo indipendenti la congiunta è uguale al prodotto delle marginali, matematicamente parlando:

$$L\left( \mathbf{\beta} \right) = \prod_{i = 1}^{N}{P(y^{(i)}|\mathbf{x}^{(i)};\mathbf{\beta})} = \prod_{i = 1}^{N}{f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right)^{y^{(i)}}\left( 1 - f_{\mathbf{\beta}}\left( \mathbf{x}^{(i)} \right) \right)^{{1 - y}^{(i)}}}$$

Ho una produttoria, utilizzo il logaritmo per farla diventare una sommatoria, lo prendo negativo così minimizzare questo significa massimizzare il logarimo positivo. Ottengo quindi una funzione di costo J definita come:

$$J\left( \mathbf{\beta} \right) = - \sum_{i = 1}^{N}{\lbrack y^{(i)}\log{\sigma\left( {\mathbf{\beta}^{T}\mathbf{x}}^{(i)} \right)}} + \left( 1 - y^{(i)} \right)\log{(1 - \sigma\left( \mathbf{\beta}^{T}\mathbf{x}^{(i)} \right)})\rbrack$$

Questa funzione è complessa e differenziabile, non ammette una soluzione in forma chiusa ma può essere risolta con metodi di ottimizzazione iterativi (come la SGD o il metodi di Newton)

## Interpretare i parametri

### Come interpetiamo l'intercetta $\beta_0$?

Il regressore logistico è utilissimo in statistica, nonchè in machine learning. Per capire come interpetare i coefficienti ricordiamo che il logit è:

$$
\log(\frac{p}{1-p})=\beta_0 + \beta_1 x
$$

Se x=0 allora il logit è:

$$
x=0 \Rightarrow \log \frac{p}{1-p}={\beta}_0
$$

Se lo andassi ad esponenziare:

$$x=0 \Rightarrow \frac{p}{1-p}=e^{\beta_0}$$

### Come interpetiamo $\beta_1$?

Come cambia il logit se mi muovo di una unità?

$$\log odds(p|x+1) - \log odds(p|x) = \beta_1$$

Per le proprietà dei logaritmi:

$$\frac{odds(p|x+1)}{odds(p|x)} = e^{\beta_1} $$

Immaginando di avere beta_1 = 0.3 esponenziandolo otteniamo 1.35, ci aspettiamo che spostandoci di una unità sull'asse delle x aumenta l'odd della positività del 35%