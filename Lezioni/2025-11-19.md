# Mercoledì 19 novembre 2025 - Classificatore Naive Bayes

Abbiamo già visto la differenza tra classificatori discriminativi e generativi, i primi decidono di modellare direttamente il decision boundary, se non sono probabilistici cercano di trovare una regola di decisione, se sono probabilistici stimano la probabilità condizionale $P(Y=k|X=x)$. Non deve essere necessariamente essere lineare, Naive Bayes non è infatti lineare. Nel caso di un classificatore Generativo vogliamo stimare la probabilità congiunta $P(X,Y)$ per fare classificazione.

## L'approccio generativo

Stimare la probabilità congiunta non è banale, la possiamo fattorizzare come:

$$
P(X,Y)=P(X|Y)P(Y)
$$

- P(Y) è facile da calcolare sono valori discreti, basta calcolare le frequenze relative
- P(X|Y) è più difficile da calcolare ma più facile della congiunta, un approccio è suddividere in due gruppi P(X | Y=0) e P(X | Y=1) sono distribuzioni di probabilità diverse più facili da trovare, ho due campioni univariati e non più un campione bivariato, posso fare il fit di entrambe le distribuzioni con una Gaussiana con mu0 e sigma0 e mu1 e sigma1. Trovare i parametri mu e sigma è facile per le due distribuzioni univariate? SI! Uso gli stimatori della popolazione dato il campione sia della media che della deviazione standard. Facendo il fit delle Gaussiane sono riuscito a calcolare la congiunta $P(X,Y)$

## Maximum a Posteriori (MAP)

Supponiamo di avere la probabilità che trova un classificatore discriminativo $P(Y=k|X=x)$ una scelta ragionevole è prendere la classe più probabile:

$$h(\mathbf{x}) = \arg \max_{k} P(Y=k | X=\mathbf{x})$$

Se applichiamo il teorema di Bayes a questo caso:

$$h(\mathbf{x}) = \arg \max_{k} \frac{P(X=\mathbf{x} | Y=k) P(Y=k)}{P(X=\mathbf{x})}$$

Nel teorema di Bayes questi termini hanno termini ben precisi:

- **Likelihood:** $P(X=\mathbf{x} | Y=k)$
- **Probabilità a Posteriori:** $P(Y=k | X=\mathbf{x})$
- **Probabilità a Priori:** $P(Y=k)$
- **Evidenza:** $P(X=x)$

Posso semplificare la formula, essendo sempre $P(X=x)$ una costante possiamo eliminarla. Ma perchè ha senso toglierlo? Ha un vantaggio computazionale 

$$h\left( \mathbf{x} \right) = \arg \max_{k} \underbrace{P( X | Y=k )}_{\text{Likelihood}} \underbrace{P(Y=k)}_{\text{Prior}}$$

## Stimare la probabilità a priori P(Y)

Posso fare delle proporzioni sul dataset, prendere statistiche del mondo reale ed effettuare assunzioni uniformi. Ma in ogni modo stimare il priori è facile

## Stimare la likelihood

Stimare la likelihood è una challenge, l'idea è raggruppare tutti i punti filtrando per y = k. Il modo in cui faccio questa stima è ciò che cambia tra i diversi algoritmi, anche qui abbiamo il problema della maledizione della dimensionalità se aumentano le dimensioni le cose si complicamo.

## Esempio: Spam Detector

Devo estrarre delle features dalle mail, utilizzo una rappresentazione chiamata Bag of Words che prende un corpus di documenti, fare preprocessing e contare il numero di volte in cui appaiono le parole ottengo rappresentazioni sparse e dimensionalità alta, esattamente le condizioni in cui KNN fa fatica.

## Il modello ideale per dati discreti

Se ho degli eventi binari, costruisco una matrice di contingenza, anche qua problema del curse of dimensionality non è una strada praticabile spesso, va bene solo se ho poche features. Esiste un walk-around che è il Naive Bayes lo vedremo tra poco.

## Il modello ideale per dati continui (QDA)

Stimo la likelihood con una Gaussiana il modello che ottengo prende il nome di Quadratic Discriminant Analysis:

$P(X=\mathbf{x} | Y=k) = N(\mathbf{\mu}_k, \mathbf{\Sigma}_k)$.

Questo classificatore quando i dati sono semplici può essere usato ma ha un problema computazionale, quanti parametri mi servono? Supponendo dimensionalità D e numero di classi K, la media è un vettore che ha dimensionalità D, una guassiana monodimensionale avrà dimensione 1, su 3 dimensioni abbiamo un elissoide, la media rappresenta il centro di questa figura. Per ogni classificatore avrò bisogno quindi di DxK parametri per la media. Per calcolare $\mathbf{\Sigma}_k$ ho bisogno di DxD parametri (Matrice di covarianza) circa la metà degli elementi sono uguali quindi possiamo ottenere $\frac{(D \cdot D)}{2} \cdot k$.

Un'altra riflessione che possiamo fare è quanto sarà stabile calcolare una matrice di covarianza con dimensioni grandi e tante classi, mi porta sicuramente a fare overfitting soprattutto quando ho una dimensionalità molto più grande rispetto al numero di elementi. Se ho un dataset piccolo il tutto funziona, ma su dataset più grandi non funzionerebbe bene.

Semplifichiamo QDA...

## Semplificazione Linear Discriminant Analysis (LDA)

Rendiamo questo modello lineare cercando di ridurre il numero di parametri, la cosa che faccio è rendere sigma uguale per tutti i miei k. Riduco la complessità di un fattore k.

$$\mathbf{\Sigma}_k = \mathbf{\Sigma}$$

E quindi:

$$P(X=\mathbf{x} | Y=k) = N(\mathbf{\mu}_k, \mathbf{\Sigma})$$

Ho come vantaggio una maggiore stabilità ma è sempre un incubo da calcolare, ho sempre una matrice quadratica, è più stabile ma non risolve il mio problema computazionale. L'unica conseguenza è che il mio decision boundary diventa lineare.

Il bias rispetto a QDA è più alto, la varianza è più bassa essendo più stabile.

## L'assunzione Naive

Risolve il problema della dimensionalità, questa assunzione che stiamo per fare è spesso sbagliata ma nella maggior parte dei casi no. L'assunzione che faccio si chiama **indipendenza condizionata**

$$X_{i}\bot X_{j}\ |\ Y,\ \forall i \neq j$$

Due variabili sono indipendenti quando conoscere il valore di una variabile non mi dice nulla sul valore dell'altra variabile. Il concetto di indipendenza condizionata è diverso lo abbiamo in tutti quei casi in cui due variabili sono dipendenti ma se suddividessimo in gruppi diventano indipendenti come ad esempio l'altezza di una persona ed il vocabolario conosciuto da ciascuna persona. Due bambini di 8 anni uno alto ed uno basso grossomodo avranno lo stesso vocabolario.

In generale:

$$ V \not\perp H$$

Ma è vero:

$$V \bot H | A$$

Il nostro problema è stimare:

$$P\left( X_{1},\ldots,X_{n} \middle| Y \right)$$

Questa cosa si traduce e si semplifica in:

$$P\left( X_{1},\ldots,X_{n} \middle| Y \right) = P\left( X_{1} \middle| Y \right)P\left( X_{2} \middle| Y \right)\ldots P(X_{n}|Y)$$

Adesso devo solo calcolare n Gaussiane multivariate ognuna di esse ha bisogno di 2 numeri reali. Il numero di parametri è dato da:

$$
2 \cdot N \cdot K
$$

Molto irrisorio rispetto alle condizioni di QDA. Questo algoritmo tende ad avere varianza più bassa.

## Perchè l'assunzione è Naive?

Se prendo due features a caso non è sempre vero che Naive Bayes funzioni correttamente, su dataset piccoli QDA va molto meglio di Naive Bayes, applichiamo Naive Bayes quando abbiamo dataset di grandi dimensioni. 

## Semplificazione della regola MAP

Con l'assunzione di indipendenza condizionata la regola MAP si semplifica in:

$$f(\mathbf{x}) = \arg_{k}max\ P\left( \mathbf{x}_{1} \middle| Y=k \right)\left( \mathbf{x}_{2} \middle| Y=k \right)\ldots P\left( \mathbf{x}_{n} \middle| Y=k \right)P(Y=k)$$

Esiste la versione:
- Gaussiana di Naive Bayes: In caso di features continue
- Multinomiale di Naive Bayes: In caso di conteggi di valori discreti

## Esempio di Gaussian Naive Bayes

Supponiamo di voler classificare uomini e donne sulla base di peso ed altezze, un classificatore QDA andrebbe a stimare una correlazione tra i dati, Naive Bayes trova due Gaussiane una per le altezze una per i pesi, in realtà ne trova 4 diverse in quanto 2 saranno per le donne e 2 per gli uomini. I parametri quindi sono $2 \cdot 2 \cdot 2 = 8$

Le 4 Gaussiane sono quindi:

-   $P\left( H = h \middle| C = 0 \right) = N(x;\mu_{1},\sigma_{1})$;

-   $P\left( W = w \middle| C = 0 \right) = N(x;\mu_{2},\sigma_{2})$;

-   $P\left( H = h \middle| C = 1 \right) = N(x;\mu_{3},\sigma_{3})$;

-   $P\left( W = w \middle| C = 1 \right) = N(x;\mu_{4},\sigma_{4})$;

Assegnerò il mio nuovo elemento alla classe 1 in base allo score dei prodotti:

$P\left( h \middle| C = 1 \right)P\left( w \middle| C = 1 \right)P(C = 1) > P\left( h \middle| C = 0 \right)P\left( w \middle| C = 0 \right)P(C = 0)$;

## Implicazioni dell'assunzione Naive

Dal momento in cui diciamo che le features sono indipendenti assumiamo che la matrice di covarianza si riduca solo alla diagonale principale, questo significa che le mie distribuzioni sono allineate agli assi. Nonostante tutte le semplificazioni il decision boundary non è lineare. Non viene inoltra assunta una correlazione all'interno della classe se c'è una forte correlazione tra le features Naive Bayes se lo perde.

Se volessimo ordinare i classificatori per bias più basso:

1. QDA
2. Gaussian Naive Bayes
3. LDA

## Naive Bayes Multinomiale

Rappresentiamo il conteggio di parole con: 

$$\mathbf{x} = (x_{1},x_{2},x_{3},\ldots,x_{d})$$

Vogliamo modellare:

$$P(x_{1}, x_{2}, \ldots, x_{d} | Y=k)$$

Possiamo riscriverla come:

$$
P(\mathbf{x} | Y=k) = \frac{(\sum_{i=1}^d x_{i})!}{x_{1}!\ldots x_{d}!} \cdot p_{k1}^{x_{1}} \cdot p_{k2}^{x_{2}} \cdot \ldots \cdot p_{kd}^{x_{d}}
$$

Il mio criterio è sempre quello di costruire il classificatore prendendo il massimo:

$$
h(\mathbf{x}) = \arg \max_{k} P(Y=k) \cdot \frac{(\sum_{i=1}^d x_{i})!}{x_{1}!\ldots x_{d}!} \prod_{i=1}^d (p_{ki})^{x_{i}}
$$

Ho degli esponenziali, applichiamo il logaritmo per semplicità.

## Stimare le probabilità per ogni parola

Lo facciamo con:

$$
p_{ki} = \frac{\text{Total count of word } i \text{ in all documents of class } k}{\text{Total count of *all words* in all documents of class } k}
$$

Oppure:

$$
p_{ki} = \frac{\sum_{j} [y^{(j)} = k] x_i^{(j)}}{\sum_{l=1}^d \left( \sum_{j} [y^{(j)}=k] x_l^{(j)} \right)}
$$

La probabilità a priori la calcolo come sempre:

$$
P(Y=k) = \frac{\sum_{j} [y^{(j)} = k]}{N}
$$

Abbiamo un problema, se una parola non appare mai nel training set io stimerò 0 ma se appare nel test set, nella mia catena di valori ho uno 0 che renderebbe tutta la catena 0.

La soluzione è effettuare lo smoothing Laplaciano

## Laplace (Add-one) Smoothing

$$
p_{ki} = \frac{\left( \sum_{j} [y^{(j)}=k] x_i^{(j)} \right) + 1}{\left( \sum_{l=1}^d \sum_{j} [y^{(j)}=k] x_l^{(j)} \right) + |V|}
$$

Il nostro modello diventa quindi:

$$
h(\mathbf{x}) = \arg \max_{k} \left( \log P(Y=k) + \sum_{i=1}^d x_i \cdot \log p_{ki} \right)
$$