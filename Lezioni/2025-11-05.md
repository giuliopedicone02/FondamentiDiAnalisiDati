# Mercoledì 5 novembre 2025 - Introduction to Predictive Analysis

Passare dalla descrizione alla predizione, finora abbiamo solo decritto i dati abbiamo utilizzato l'inferenza statistica per capire se le nostre affermazioni erano valide o meno. I modelli predittivi ha principalemte due utilizzi:

- Inferenza: utilizzare elementi predittivi per comprendere meglio i dati 
- Predizione: Voglio fare predizioni su dei dati non visti o magari parziali, ad esempio voglio predire qual è la probabilità che una persona sviluppi una malattia sulla base di altri fattori

## Dataset di esempio: Pima Indians Diabetes Dataset

Per ogni osservazione abbiamo:

- Numero di gravidanze: Discreta numerica ordinale ratio
- Livello di glucosio: Continua
- Pressione sanguigna: Continua
- Spessore della pelle: Continua
- Insulina: Continua
- BMI: Continua
- Età: Continua
- Outcome: Variabile binaria categorica che indica la presenza o meno del diabete

## Cos'è un modello

Un modello è una rappresentazione semplificata della realtà se consideriamo una mappa geografica è una rappresentazione semplificata della realtà è utile proprio perchè è una semplificazione questo però significa che non sono molto accurati. Una citazione di George Box ci dice che tutti i modelli sono sbagliati ma alcuni di questi sono utii (perchè li possiamo utilizzare per i nostri scopi). Non esiste infatti un modello perfetto, non avrò mai le features perfette.

## Modellare la distribuzione del BMI

Immaginiamo di prendere la colonna BMI e cerchiamo di fittare i dati con una Gaussiana sovrapposta all'istogramma, il vantaggio è che stiamo riassumendo 768 dati in due numeri **media** e **deviazione standard** il 67.7% dei dati sta tra 25 e 40 semplicemente sottraendo e sommando una deviazione standard dalla media.

Se volessimo calcolare quante persone abbiamo tra un BMI di 45 e 50 possiamo calcolare la CDF(50) - CDF(45) o tramite l'integrale definito tra 45 e 50.

Ricordiamoci tuttavia che il modello è sbagliato assume che esistano valori di BMI negativi, esistono dei limiti biologici sotto il quale il BMI non esiste, non esiste un BMI inferiore a 20 ma se volessimo calcolare la CDF(20) in realtà con questa approssimazione della normale non uscirebbe 0.

## Il framework generale dei modelli predittivi

Y è la variabile che io voglio predire si chiama **variabile predetta** o **dipendente**, X sono le **variabili predittorie** ched io utilizzo per predire un'altra variabile.

la formula generale dei modelli predittivi è:

$$
Y = f(X) + \epsilon
$$

- f(x) è una funzione con una forma analitica
- epsilon rappresenta il termine di errore nella predizione come la causalità o fattori che non sono stati misurati

Nel caso del nostro esempio vorremmo predire i valori di insulina dalla concentrazione del glucosio, tramite uno scatterplot tra insulina e glicemia tracciamo una linea che è una possibile candidata per f(x), ma per il momento è solo una stima, ne parleremo più avanti. Non è un modello molto accurato se considerassimo 150 di glucosio otteniamo più punti in quell'asse e infatti da uno scatterplot non riusciamo a costruire una funzione in quanto non è vero che ad ogni x corrisponde un solo y. La linea rossa è invece una funzione che sembra essere una "media" ed è più utile rispetto allo scatterplot in se.

Invece di dire che l'insulina ha esattamente questo valore il modello ci predice un valore medio di insulina dato un certo valore di glucosio, possiamo anche restituire un intervallo.

## Due obiettivi: Capire e Predire

Possiamo chiederci dove è più difficile predire l'insulina? Dove abbiamo più dispersione di dati? Quando rispondiamo a queste domande sto **capendo** meglio i dati per fare ragionamenti di più alto livello, altre domande sono relative al trend.

Il secondo obiettivo è quello **predittivo** succede quando le mie features non sono interpretabili come le intensità dei grigi nelle scale di colori RGB.

Nel nostro caso vorremmo cercare di **capire** quali sono i fattori principali che portano al diabete e voler **predire** quali pazienti possono sviluppare il diabete nei prossimi anni.

## Statistica VS Machine Learning

L'approccio statistico lo possiamo considerare come un approcci di tipo glass-box il mio modello lo conosco a pieno come la Gaussiana e so tutte le sue caratteristiche dal punto di vista analitico, ci permettono di utilizzare test di ipotesi, p-value, test di confidenza ed il mio obiettivo è quelo di **interpretare il modello**. la domanda che mi pongo è sed la relazione che ho è reale o è dovuta al caso.

L'approccio relativo al machine learning è invece un approccio di tipo black box, in questo caso ci interessa ridurre le assunzioni, è inoltre un approccio result-driven, dividiamo il dataset in Train Test e Validation perchè l'approccio statistico complicherebbe il modello a favore dell'interpretabilità, nel machine learning invece perdo l'interpretabilità. Quando il mio modello non è interpretabile come faccio a stabilire un test di ipotesi? Non posso più fare test statistici in caso di modelli complessi, non ho quindi più garanzie teoriche in maniera empirica dico che se il modello funziona bene su dati che non ha mai visto ci dà una garanzia sul fatto che il modello abbia buone performance. La domanda chiave è: quanto sono accurate le predizioni sui nuovi dati?

## Trade-off tra complessità ed interpretabilità

Man mano che aumentiamo la complessità del modello perdiamo l'interpretabilità del modello, la regressione lineare la vedremo dal punto di vista statistico, man mano che ci spostiamo verso il deep learning con modelli sempre più complessi diminuisce l'interpretabilità.

Nella reressione lineare y= ax + b a e b sono facilmente interpretabili in quanto indicano lo slope e lo spostamento rispetto all'asse delle x, modificando quei valori ci aspettiamo di ottenere qualcosa che conosciamo bene. Nel caso di modelli quadratici o ancora più complessi diventa molto complicato interpretare il risultato.

Questo trade-off è fortemente associato al trade-off bias-varianza.

## Supervised vs Unsupervised Learning

Il supervised learning è la branca del machine learning più sviluppata e più facile da capire, parliamo di supervised learning ogni volta che parliamo di modelli predittivi o più in generale quando abbiamo un insieme dei dati che contiene sia le mie variabili X i predittori sia i risultati corretti di predizione Y

$$\hat{y} = h(X)$$

h(x) vuol dire modello predittivo

Nel caso di unsipervised learning il dataset contiene solamente le features in input x.

Esistono due cateogire di Supervised Learning: Regressione Classificazione.

## Regressione: Predizione di numeri

Vogliamo trovare una funzione che da valori continui permetta di ottenere un'altra variabile continua. Nel nostro caso vogliamo predire dato il numero di metri quadri il prezzo di una casa.

Esempi sono predire i prezzi delle case dai loro metri quadri, le previsioni del meteo di domani, stimare il risultato di un esame date le ore di studio.

Uno statista Galton mettendo sull'asse delle x l'altezza dei genitori e sull'asse delle y le altezze dei figli riesco a trovare un trend che genitori che avevano un'altezza sopra la media tendevano ad avere figli con un'altezza sotto la media e viceversa genitori con un'altezza sotto la media tendono ad avere un'altezza sopra la media. Questo lo chiamò **regressione verso la media** e da qui nasce la teoria della regressione lineare.

## Classificazione: Predizione di categorie

Ho una funzione f definita in omega con valori in E dove E è un insieme discreto che contiene n classi o gruppi, la domanda che ci chiediamo è "Questo è A oppure B?". Esempi di classificazione binaria sono quello di capire se una mail è spam o meno, diagnosi mediche, i casi multiclasse sono quelli di image recognition, sentiment analysis.

In generale ho in input due features X1 ed X2 e da queste due variabili voglio ricostruire una variabile binaria.

I problemi linearmente separabili sono tutti quelli dove esiste una retta in grado di separare due classi, quelli che si trovano sopra la retta e quelli che si trovano sotto la retta.

## Unsupervised Learning: Trovare strutture nascoste

Un esempio è quello del clustering in maniera del tutto autonoma l'algoritmo deve essere in grado di dividere in gruppi omogenei suddividendo lo spazio in più parti, è utile per segmentare i clienti di una banca in gruppi sulla base di età e spending-scores ottenendo young-savers old-savers e prime-spenders, ogni volta che ho un nuovo cliente lo posso immediatamente categorizzare per proporre un pacchetto di servizi più adatto a lui. Simile al clustering è la density estimation.

La riduzione di dimensionalità consiste nel ridurre il numero di variabili preservando le informazioni essenziali

Anomaly Detection avviene quando abbiamo comportamenti del tutto normali e vogliamo capire i casi che rispecchiano casi non standard come ad esempio le transazioni fraudolente.

## Modelli parametrici e non parametrici

I modelli parametrici si basano su dei parametri, nel caso della retta y=ax+b, a e b sono detti parametri del modello, faccio delle assunzioni sui dati e assumo che i dati si distribuiscono in modo lineare il mio obiettivo è solo trovare la retta giusta. Il vantaggio è quello di ottenere un modello molto compatto ed ho bisogno di meno dati in quanto voglio stimare pochi dati, sono semplici e veloci da allenare e sono interpretabili. Sono tuttavia limitate dalle assunzioni nel nostro caso l'assunzione di linearità se i nostri dati non sono distribuiti in modo lineare ma in modo parabolico l'approccio non funzionerà. Soffrono spesso di underfitting non si adatta bene ai dati.

Un modello non parametrico è un modello molto flessibile che cattura pattern complessi che ha meno bias (errore sistematico minore) richiede più dati è più lungo da allenare e soffre di overfitting, apprende troppo bene dai dati e potrebbe non funzionare su dati non visti in precedenza. Un esempio è il K-Nearest Neighbours.

## Empirical Risk Minimisation

Framework che viene fuori dallo statistical learning, definiamo due spazi di oggetti X e Y che sono i nostri input ed in ostri output, sono i sample spaces di due variabili aleatorie dove X sono i predittori e Y la variabile predetta (ground truth). Il nostro obiettivo è di trovare una funzione chiamata (ipotesi) h definita in valori da X a Y

$$h : \mathcal{X} \to \mathcal{Y}$$

il cui output rappresenta la predizione

$$\hat y = h(x)$$

Nel caso di diagnosi medica abbiamo un problema di classificazione binaria, vogliamo trovare una funzione che dagli esami del sangue mi permette di capire se una persona ha una patologia o meno. Altro esempio è quello di filtraggio delle e-mail dove f(x) conta il numero di errori ortografici presenti in una mail, in effetti i primi filtri anti spam segnalavano come spam tutte quelle email che hanno errori ortografici, una mail con diversi errori ortografici non veniva segnalata come spam 

## Cosa apprendono i modelli

In un modello parametrico definiamo una famiglia di soluzioni, in caso di classificazione binaria tutto ciò che è minore di theta viene classificato in classe 0 se maggiore in classe 1, theta prende il nome di **threshold** ed è un parametro, ma come faccio a trovare i parametri migliori?

## Approccio di apprendimento: Aprrendimento statistico e rischio empirico

Anche definito come fit, un modo di farlo è quello di quantificare un rischio empirico che ci permette di capire che rischio corriamo quando definiamo una soluzione. Definiamo inizialmente la funzione di perdita anche detta funzione di costo che prende in input il valore predetto ed il valore da predire e ci dice qual è il costo che affrontiamo quando ci affidiamo al predittore

$$L(\hat y, y)$$

Una semplice funzione di Loss potrebbe restituire 1 se il valore predetto è diverso dal valore da predire 0 altrimenti, sulla base di questo costo vorrei trovare qual è il costo medio e quindi il rischio:

$$R(h) = E_{(x,y) \sim P(X,Y)}[L(h(x),y)]$$

cerco il valore atteso su tutte le funzioni di Loss tra il modello e la nostra ground-truth. Ma come la troviamo P(X,Y) è una distribuzione qualsiasi, posso stimarla dai dati ma non è sempre possibile, chi ce lo dice che i dati seguono davvero una distribuzione... non è quindi il modo corretto di procedere.

Definiamo quindi il rischio empirico, se ho un campione sufficientemente rappresentativo della popolazione (training set) $\text{TR}=\{(x_i,y_i)\}_{i=1}^N$ allora posso definire il rischio empirico come la media ddi valori di L

$$R_{emp}(h) = \frac{1}{N}\sum_{i=1}^N L(h(x_i),y_i)$$

Quello che voglio ottenere è la soluzione h che minimizza il rischio empirico da una famiglia H

$$\hat h = \underset{h \in \mathcal{H}}{\mathrm{arg\ min}}\ R_{emp}(h)$$

I passaggi sono quindi 3:

1. Calcolare una funzione di Loss
2. Calcolare il rischio empirico
3. Minimizzare il rischio

## Capacità del modello: Flessibilità vs Compatibilità

I modelli che hanno uno spazio di capacità bassa sono semplici e vincolati, hanno uno spazio di ipotesi piccolo, ma portano ad underfitting.

I modelli che hanno capacità più alta possono risolvere problemi più complessi sono più flessibili ma si portano dietro il rischio di overfitting

## Misurare l'accuratezza del modello

Per fare questo si utilizzano le misure di performance o misure di valutazione definite caso per caso a seconda del problema che voglio risolvere, nei problemi di regressione spesso si utilizza l'MSE

$$R_{emp}(h) = \frac{1}{N}\sum_{i=1}^N (y_i - h(x_i))^2 = \text{MSE}$$

Diventa quindi una quantificazione del rischio empirico, ridurre l'MSE è una delle misure più utilizzate sia come Loss che rischio empirico.

## Overfitting e Underfitting

Consideriamo il modello:

$$\hat h(x) = y \text{ s.t. } (x,y) \in \text{TR}$$

Definiamo il rischio empirico come:

$$R_{emp}(\hat{h}(x)) = \frac{1}{N}\sum_{i=1}^N L(y,y) = 0$$

Questa funzione porta rischio empirico nullo, ma minimizza anche il rischio? No perchè ci sono elementi che non appartengono al training set ed in quel caso non avrei rischio nullo, questo è il problema dell'**overfitting** ho un errore bassissimo o nullo sul training set ma pessime performance sul test set.

Vogliamo trovare un modello di regressione che a valori di x associa valori di y, i punti blu rappresentano il training set i punti rossi rappresentano il test set.

Nel caso di un regressore lineare, trova una retta che ha un fit solo per i punti blu, l'MSE è alto questa retta non passa per nessun punto del training set non è abbastanza flessibile, siamo in **underfitting**. Scegliamo quindi una funzione più complessa con un polinomio di grado 5, più sono i gradi più sono i punti di flesso troviamo una soluzione che passa per tutti i punti del training set ed ha MSE sul training set = 0, ma l'MSE sul test set è ancora peggiore del caso precedente, questo si chiama **overfitting**. Il modello quadratico è una buona soluzione.

Il Bias che indica l'errore sistematico nel caso di underfitting è alto, la varianza la posso calcolare togliendo un punto dal training set e capire come il modello cambierà in caso di underfitting la varianza è bassa. Nel caso di overfitting il bias è molto basso ma ha una varianza alta se tolgo un punto dal training set il risultato cambierà parecchio.

## Trade-off tra Bias e Varianza

Il valore atteso dell'errore, dato un punto x0 e un associato y0 il valore atteso dell'errore tra valore predetto e da predire è uguale al bias al quadrato + la varianza di f + la varianza del termine di errore ineliminabile:

$$E[(y_0 - \hat{f}(x_0))^2] = [\text{Bias}(\hat{f}(x_0))]^2 + \text{Var}(\hat{f}(x_0)) + \text{Var}(\epsilon)$$

Se riduco il bias aumenta la varianza e se riduco la varianza aumento il bias, se provo a mettere a 0 la varianza il bias resta alto. Voglio trovare una complessità che minimizza l'errore totale.

## Parametri ed Iperparametri

Il processo di learning trova i parametri ma non gli iperparametri, sono dei parametri esterni come il grado del polinomio

## Strategie di Model Selection

Come facciamo a capire qual è il grado del polinomio migliore? Quello che minimizza l'errore sul test set. Generalmente suddiviso il dataset in più set uno per il teaining uno per gli iperparametri ed una per vedere le misure di performance

## Strategie di Model Selection

Dividiamo in training set e test set senza usare il validation set, per prima cosa facciamo uno shuffle del dataset per evitare distribution shift e quindi problemi di overfitting, il 70% di questo dataset diventa il nostro training set il 30% il nostro test set. Possiamo scegliere percentuali differenti, ma ricordiamo che lo standard error è inversamente proporzionale alla radice di n, se abbiamo dataset molto grandi va bene anche una suddivisione 50/50 ma nella maggior parte dei casi si usa 70/30

## Cross Validation

Se ho un dataset piccolo anche se faccio tutto in maniera random, invece di avere un solo split divido il dataset in un certo numero (k) di folds con shuffle random. Nel primo split considero il primo fold come test i restanti 3 come training, nel secondo split considero il secondo fold come test ed il primo, il terzo ed il quarto fold come training set...

## Leave-One-Out-Cross-Validation

Versione estrema del Cross Validation per dataset molto piccoli, ma faccio coincidere k con il numero n di elementi, prendo un singolo elemento come test set ed il resto come training set, si fa solo quando ho davvero pochi dati

## Model Selection con gli Iperparametri

Quando ho iperparametri ho bisogno anche di un terzo dataset chiamato Validation Set, una buona suddivisone è 60/20/20, è utile per capire il grado del polinomio senza utilizzare il test set, quando ho più parametri si fa di solito una ricerca a griglia esistono delle tecniche di ottimizzazione Bayesiana che mi permettono di ottimizzare i punti da calcolare sulla griglia, ma non fanno parte di questo corso.