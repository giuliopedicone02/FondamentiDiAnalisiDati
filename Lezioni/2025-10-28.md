# Martedì 28 novembre 2025 - Distribuzione dei dati

Strumenti molto utili nell'analisi dei dati e in tutto ciò che ha a che fare con i dati.

Una distribuzione di probabilità è una funzione f definita nel nostro spazio degli eventi omega in valori [0,1], una funzione che preso un evento associa un valore di probabilità.

> Quando i valori di una variabile X seguono una distribuzione P, si dice che X segue P

Esistono due distribuzioni di probabilità:

- Nel caso delle variabili discrete le distribuzioni di probabilità si chiamano PMF (Probability Mass Function)
- Nel caso delle variabili continue le distribuzioni di probabilità prendono il nome di PDF (Probability Density Function)

## Probability Mass Function (PMF)

Affinchè la PMF sia valida deve valere che la somma delle sue probabilità sia uguale ad 1

Esempi: 

- Lancio di una moneta bilanciata, abbiamo un numero finito di valori di probabilità infatti Omega = {head,tail} e P(head) = 0.5 e P(tail)=0.5 e P(head) + P(tail) = 1. Viene rappresentato da un grafico puntuale non è una curva

- Lancio di una moneta truccata, utilizziamo un approccio frequentista immaginiamo che esce 6000 volte testa e 4000 volte croce, quindi P(head) = 0.6, P(tail)=0.4 e P(head) + P(tail) = 1.

In Python otteniamo la PMF utilizzando `.value_counts(normalize=True)`

## Cumulative Distribution Function (CDF)

Una funzione cumulativa, o funzione di ripartizione è la probabilità che la variabile X abbia un valore minore o uguale di x detta anche come la somma delle probabilità di tutti gli y minori o uguali di x. Si indica con la lettera F(x), ad esempio F(180) è la probabilità di trovare un individuo più basso di 180 cm.

In generale se volessi calcolare la probabilità che x è compreso tra a e b dobbiamo calcolare la differenza tra F(b) - F(a)

$$
P(a\leq x \leq b) = F(b) - F(a)
$$

Nel lancio di un dado la PMF si ottiene con la probabilità di ottenere un singolo evento quindi 1/6 mentre la CDF si ottiene sommando le probabilità tramite somma cumulativa (1/6, 2/6, 3/6, ..., 1).

In termini teorici prende il nome di CDF quando è calcolata dai dati prende il nome di ECDF (Empirical Cumulative Distribution Function).

## Probability Density Function (PDF)

Quando parliamo di variabili continue, ci chiediamo invece della probabilità di trovare una persona che sia alta esattamente 180cm ci chiediamo come calcolare la probabilità che una persona sia compresa tra 179 e 181cm, quindi cerchiamo la probabilità di trovarci in un intorno del valore cercato.

Prendo un intorno cerco di farlo diventare sempre più piccolo fin quando il raggio del mio intorno tende a zero, questa quantità la indico con f(x) e la chiamo densità.

Quando lavoro con le variabili continue parlo quindi di densità e non probabilità come per le variabili discrete, la funzione di densità è una funzione definita in Omega con valori in [0,1].

La probabilità che x appartiene ad un intorno e quindi che x sia compreso tra a e b è ugualoe all'integrale definito tra a e b della funzione di densità, inoltre l'integrale indefinto tra -inf e +inf è uguale ad 1. P(-inf <= x <= +inf) = 1.

## Distribuzione Uniforme

$$
f(x) = \frac{1}{b-a}
$$

Funzione che definisce un determinato fenomeno dove tutti gli elementi dello spazio campionario hanno la stessa probabilità

## Approssimare una PDF dai dati

Possiamo ricavare una PDF dai dati, ma non la possiamo considerare una densità ma otteniamo qualcosa di frastragliato parlando appunto di istogrammi a barre, affinchè l'istogramma possa essere interpretato come una distribuzione l'area della nostra curva deve essere uguale ad 1.

Se normalizzo l'istogramma dividendo per n non è detto che otteniamo che l'area sia uguale ad 1, in generale normalizzare l'istogramma non aiuta, posso definire l'altezza delle mie barre come il numero di elementi che ricadono in un bin fratto lo spessore di ogni barra per il numero di elementi. Questo istogramma definito in questa maniera si chiama **istogramma di densità**

## Cumulative Distribution Function (CDF)

Possiamo definire la CDF per le discrete allo stesso modo delle continue ma sostituendo le sommatorie con gli integrali.

La CDF della distribuzione uniforme può essere definita come:

- per x<a: F(x) = 0
- per a<x<b = $\frac{x-a}{b-a}$
- per x>b F(x) = 1

## Distribuzioni di probabilità comuni

Sono distribuzioni di probabilità molto utilizzate per assunzioni sui dati e costruire algoritmi che possono apprendere dai dati

## Discrete Uniform Probability Distribution

La distribuzione discreta uniforme dato uno spazio campionario di k elementi è definita come

$$
P(X=a_i) = 1/k
$$

## Distribuzione di Bernoulli

Definita su una variabile con output binario come ad esempio head o tail mappati in 0 e 1, questa distribuzione dipende da un parametro $\phi \in [0,1]$, questo parametro ci dice quanto questa distribuzione è bilanciata o meno ed indica la probabilità P(X=1) ed indica la probabilità di un successo. Scegliendo $\phi = 0.5$ ottengo la distribuzione uniforme, se scelgo $\phi = 0.8$ l'80% delle volte osserverò X=1 ed il 20% delle volte osserverò X=0.

Riesce a modellare perfettamente lo studio di un lancio di una moneta

## Distribuzione Binomiale

Ci chiediamo qual è la probabilità che lanciando n monete o lanciando n volte la stessa moneta ottengo esattamente k successi. Ad esempio lanciando una moneta 3 volte qual è la probabilità di ottenere esattamente 2 successi.

$$
P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}
$$

Dove:
- k è il numero di successi
- n è il numero di tentativi indipendenti
- p è la probabilità di successo di un singolo tentativo

La probabilità di ottenere 3 volte testa lanciando una moneta è uguale a:

$$
P(3) = \binom{3}{3}{0.5}^{3}(1 - 0.5)^{3 - 3} = {0.5}^{3} = 0.125
$$

Converge ad una Gaussiana per valori di n molto grandi

## Distribuzione Categorica

Generalizza quella di Bernoulli al caso in cui ho più di 2 esiti, come il lancio di un dado. La mia distribuzione è definita da una serie di parametri p1,p2,...,pk la cui somma faccia 1 e la probabilità viene definita come:

$$
P(X=i) = p_i
$$

## Distribuzione Multinomiale

Estende la distribuzione binomiale, in questo caso l'esempio pratico è il lancio di n dadi. nella multinomiale lancio n dadi e mi chiedo qual è la probabilità di ottenere un determinato vettore di esiti. La distribuzione multinomiale è definita come:

$$P\left( n_{1},\ldots,n_{k} \right) = \frac{n!}{n_{1}!\ldots n_{k}!}p_{1}^{n_{1}} \cdot \ldots \cdot p_{k}^{n_{k}}$$

Ad esempio se io lancio il dado 6 volte, n=6, qual è la probabilità che esca una volta 1, una volta 2, una volta 3, una volta 4, una volta 5, una volta 6,

Definisco n1=1, n2=1, n3=1, n4=1, n5=1, n6=1. n1+n2+n3+n4+n5+n6 = 6 = n

## Distribuzione Normale

La distribuzione normale o la distribuzione Gaussiana è una distribuzione definita da due parametri:

- media: dove la distribuzione si trova nello spazio corrisponde al picco
- varianza: rappresenta la larghezza di questa funzione, dove questa distribuzione inizia a flettere, una varianza più piccola indica una distribuzione meno incerta e quindi più stretta.

La distribuzione è simmetrica e il valore della distribuzione decresce ogni qual volta ci allontaniamo dalla media

La distribuzione normale è definita come:

$$
N\left( x;\mu,\sigma^{2} \right) = \sqrt{\frac{1}{2\pi\sigma^{2}}}e^{- \frac{1}{2\sigma^{2}}(x - \mu)^{2}}
$$

In questa formula ci sono diversi elementi, ad x sottraiamo la media dal punto di vista grafico sto misurando la distanza tra quel valore e la media ed elevo questo valore al quadrato in modo da ottenere una distribuzione simmetrica. Questo valore lo moltiplico per $\frac{1}{2\sigma^2}$ posso considerarlo un fattore di scala, se la distribuzione è stretta il mio essere distante dal centro verrà amplificato, questa distanza viene normalizzata utilizzando la deviazione standard. Questo numero lo sto negando e lo esponenziando, la funzione e è una funzione monotona crescente, quando mi discosto molto dalla media ottengo un valore molto piccolo qusndo sono vicono alla media ho valori molto grandi. Il termine $\sqrt{\frac{1}{2\pi\sigma^{2}}}$ serve in modo tale che la normale integri sempre ad 1.

## Densità della Distribuzione Gaussiana

La densità della gaussiana, se la media è nulla tra -sigma e +sigma è circa il 68%, tra -2sigma e +2sigma è circa il 95% e tra -3sigma e +3sigma è circa 99%.

Immaginando di avere un campione con media delle altezze e deviazione standard di 3, all'interno dell'intervallo [187-9, 187+9] otteniamo circa il 99% dei valori.

## Teorema del Limite Centrale

Questo teorema ci dice che la somma o la media di variabili indipendenti tende ad una distribuzione normale indipendentemente dalla distribuzione iniziale. Supponiamo di avere n variabile aleatorie e non sappiamo nulla su come queste variabili siano distribuite. Al crescere di n con n che tende ad infinito la distribuzione tende a distribuirsi secondo una Gaussiana.

Ad esempio se lancio un dado, ho una variabile aleatoria relativa ad un dado ottengo una distribuzione uniforme, se lancio due dadi e sommo i risultati comincio ad ottenere qualcosa che tende ad essere sempre più vicino ad una distribuzione normale in quanto i valori centrali sono i più frequenti poichè posso ottenerli in più modi. Aumentando sempre di più il numero di dadi la distribuzione tenderà ad essere una Gaussiana.

Questa caratteristica ci aiuta perchè ci permette di applicare l'assunzione di Gaussianità in tantissimi casi

## Distribuzione Gaussiana Multivariata

Le distribuzioni gaussiane si generalizzano anche a più dimensioni, supponiamo di avere d dimensioni i parametri di cui ho bisogno sono:

- media
- matrice di covarianza (d x d): Calcola le distanze dei punti dal centro tenendo in considerazione la varianza

Si definisce come:

$$
N\left( \mathbf{x;\mu,}\mathbf{\Sigma} \right) = \sqrt{\frac{1}{(2\pi)^{d}\det(\Sigma)}}e^{( - \frac{1}{2}\left( \mathbf{x} - \mathbf{\mu} \right)^{T}\Sigma^{- 1}\left( \mathbf{x} - \mathbf{\mu} \right))}
$$

Nel caso di di 2 dimensioni la Gaussiana può essere più stretta in un asse e più larga nell'altro asse, tutto ciò dipende dalla matrice di Covarianza.

Una Gaussiana isotropica si comporta alla stessa maniera su tutti gli assi, ha la stessa varianza su tutti gli assi ed è simmetrica.

## Stima dei parametri di una Gaussiana

La tecnica della maximum likelihood sono quelle che massimizzano la probabilità che la Gaussiana descriva bene i dati.

## Dsitribuzioni Descrittive

Risulta essere utile descrivere le distribuzioni con alcuni statistiche come:

- media (valore atteso)
- varianza
- covarianza
- entropia

## Valore atteso

la definisco come la sommatoria di x per P(x), rappresenta una generalizzazione della media dando dei pesi ai diversi elementi, nel caso continuo sostiuisco la sommatoria con l'integrale.

Il valore che ottengo è il valore atteso ovvero quello che si trova al centro della distribuzione

## Varianza e Deviazione Standard

La varianza viene definita come X - il valore atteso (ovvero la media) al quadrato.
La deviazione standard è la radice della varianza.

## Covarianza

Valore atteso del prodotto tra gli scarti tra X ed il valore atteso di X ed Y ed il valore atteso di Y

## Self-Information

Esiste un concetto, quello dell'informazione associata ad un evento. La teoria dell'informazione ci dice che in generale abbiamo una definizione di self-information che ci quantifica il livello di informazione trasportato da un semplice evento.

- Gli eventi rari danno più informazioni di quelli più frequenti
- La self-information di eventi indipendenti deve essere additiva.

L'informazione è definita come:

$$
I(x) = -\log_2 P(x)
$$

e si misura in bit

Due eveneti indipendenti devono avere informazione additiva

## Entropia

Si associa l'entropia ad una distribuzione di probabilità ci indica quanto una distribuzione di probabilità è rumorosa o incerta, un valore alto indica molto disordine o randomicità un valore basso indica meno disordine.

Per calcolare l'entropia si guarda il valore atteso della self-information.

Viene definita come:

- Per variabili discrete:
  $$
  H(X) = -\sum_{x \in \Omega} P(x) \log_2 P(x)
  $$
- Per variabili continue:
  $$
  h(X) = -\int_{x \in \Omega} f(x) \log_2 f(x) dx
  $$

Calcolo quindi la self-information media della mia distribuzione, se la distribuzione ha un picco do molto peso a ciò che sta al centro che ha meno informazione e molto peso a ciò che sta ai lati che trasporta più informazione.

L'entropia della distribuzione di Bernoulli ha il suo picco a 0.5 essendo una distribuzione uniforme completamente randomica, agli estremi abbiamo entropia 0 diventa deterministico.

## Standardizzazione 

Lo Z-Scoring la si può applicare anche alle variabili, si definisce una nuova variabile aleatoria Z e la definisco come:

$$
Z = \frac{X - \mu_X}{\sigma_X} = \frac{X-E[X]}{\sqrt{Var[X]}}
$$

Questo ci permette di essere indipendenti dall'unità di misura e ci permette inoltre di avere distribuzioni di probabilità più comparabili. Questo ci permette di capire se la nostra distribuzione si adatta ad una distribuzione normale standard con media 0 e varianza 1.