# Probabilità dell'Analisi dei dati

La probabilità ci permette di quantificare l'incertezza.

## Che valori assumano le variabili?

Bisogna chiederci che valori assumeranno le variabili. Abbiamo due tipologie di sistemi:

- Sistema Deterministico: Prevedibilità assoluta di un fenomeno (nel macroscopico) 
- Sistemi incerti o Stocastici: Sistemi in cui non è facile predire come qualcosa si evolve nel tempo in maniera accurata, ci sarà sempre un certo grado di incdertezza causato da eventi casuali che influenzano il fenomeno analizzato

## Risorse di incertezza

Un sistema può essere inerentemente stocastico come il lancio di un dado
Ci sono casi in cui il sistema non è stocastico ma la nostra osservabilità è incompleta come il gioco di Monty Hall, il sistema è deterministico ma dobbiamo ragionare in termini probabilistici
Altro caso è la mancata capacità di modellare il sistema in modo completo, come un robot che tramite una singola fotocamera 2D vuole costruire uno schema 3D non è possibile farlo, servirebbero due videocamere ma è possibile fare assunzioni probabilistiche sulla stima della profondità.

## Perchè è importante la teoria delle probabilità?

La probabilità ci permette di quantificare gli eventi come quello del lancio di un dado, a combinare informazioni, esiste una forma di algebra della probabilità che permette di calcolare la probabilità di un event sulla base delle probabilità di altri eventi.

## Esperimenti Random

Esperimento che può essere ripetuto più volte i cui risultati possono variare, il lancio di un dado è un esperimento stocastico o aleatorio.

Terminologia:

- Spazio dei possibili risultati $\Omega$: Insieme di tutti i possibili output. Nel lancio di un dado $\Omega = {1,2,3,4,5,6}$
- Singolo evento $\omega_i$: Un singolo possibile output appartenente ad $\Omega$
- Evento $A \subseteq \Omega
$: Sottoinsieme dei possibili eventi. Esempio $A={2,4,6}$ probabilità che lanciando un dado esca un numero pari. Esiste anche il complementare $\bar{A}$

## Variabili Random / Aleatorie

La variabile aleatoria codifica i risultati di un esperimento aleatorio alla quale possiamo associare una probabilità, possono essere scalari, singolo numero, vettori, multi-dimensionali, continue...

Formalmente è una funzione $X:\Omega \rightarrow E$

## Esempio relativo al dataset del Titanic

Variabili aleatorie:

- C: Classe {1,2,3}
- S: Sesso {M,F}

Ci chiediamo se è più probabile prendere un uomo o una donna o se prendere un passeggero tra prima seconda o terza classe

## Cosa sono i dati

I dati sono i valori assunti dalle variabili aleatorie

Esempio `S=male` ma il dato è formato dalla coppia {Variabile Aleatoria, Valore}

## Nozione di Probabilità

Numero che associamo ad un evento compreso tra 0 ed 1 [0,1] dove:

- 0 indica che l'evento è impossibile
- 1 indica che l'evento è certo

La notazione utilizzata è : $P(C=1)$ rappresenta la probabilità che il passeggero scelto provenga dalla prima classe (L?evento sta dentro le parentesi tonde)

La probabilità che una persona sia alta 180cm si indica con $P(H=120)$, o la probabilità che una persona sia più bassa di 320cm si indica con $P(H\leq120)=1$

## Assiomi di Kolmogrov's

- **Assioma 1:** Non negatività: La probabilità di un qualsiasi evento è sempre compresa tra 0 ed 1
- **Assioma 2:** La probabilità del Sample Space $\Omega$ è sempre 1 
- **Assioma 3:** Additività: Se abbiamo due eventi e questi eventi sono disgiunti come A=numeri pari={2,4,6} B=numeri dispari={1,3,5} e quindi la loro intersezione è un insieme vuoto allora la probabilità dell'unione è data dalla somma della probabilità di A sommata alla probabilità di B.

La probabilità cresce in maniera additiva se gli eventi sono tra loro disgiunti. Da questi tre assiomi ricaviamo 4 corollari

- Regola del complemento: Definito come $P(\bar{A})=1-P(A)$
- Evento impossibile: La probabilità di un insieme vuoto è 0
- Regola generale della somma: $P(A \cup B) = P(A) + P(B) - P(A \cap B)
$
- Monotonicità: Se A è sottinsieme di B la probabilità che si verifichi A è minore o uguale alla probabilità di B. Se B contiene tutto ciò che contiene A vuol dire che B è più probabile di A

## Probabilità di Laplace

La prima regola definisce la probabilità nel seguente modo: Se gli eventi sono equiprobabili come estrearre una carta da un mazzo di 52 carte o lanciare un dado, definisco la probabilità di un evento come il rapporto tra casi favorevoli e casi possibili.

Ad esempio la probabilità che esca un numero pari lanciando un dado è dato da $\frac{3}{6} = \frac{1}{2}$.

Dato l'insieme {1,2,2,3,4,1,6}
- P(1) = $\frac{2}{7}$
- P(2) = $\frac{2}{7}$
- P(3) = $\frac{1}{7}$
- P(4) = $\frac{1}{7}$
- P(6) = $\frac{1}{7}$

La loro somma è $\frac{7}{7}$ quindi 1

## Approccio Frequentista

Immaginiamo di lavorare in un contesto blackbox come quello delle slot machines, posso utilizzare un approccio frequentista. L'approccio frequentista è dato dal rapporto dal numero di volte che si verifica l'evento X ed il numero di tentativi

## Approccio Bayesiano alla Probabilità

Approccio di tipo soggettivo, la probabilità viene vista come misura di incertezza o meglio quanto **credo che un evento sia improbabile**. Questo tipo di approccio è utile perchè ci sono alcuni eventi che non sono ripetibili, come quello di stimare la probabilità che il Sole si estinguerà tra 5 miliardi di anni.

## Calcolo delle proabilità dai dati

Se le variabili sono dicrete possiamo calcolare la probabilità tramite approccio frequentista ed è equivalente ad effettuare un `value_counts` e normalizzare con `(normalize=True)`

## Probabilità Congiunta

La probabilità che avvengono due eventi contemporaneamente, ad esempio chiederci qual è la probabilità di ottenere un numero pari se ottengo un numero pari lanciando due dadi.

Vale la proprietà simmetrica $P(X=x, Y=y) = P(Y=y,X=x)$

La tabella di contingenza è una matrice in cui nelle righe troviamo i possibili risultati di X nelle colonne i possibili risultati di Y. Le intersezioni sono rappresentati da numeri che indicano quante volte si verificano insieme quei due eventi. A questa matrice si aggiunge una riga ed una colonna che rappresenta la somma della rispettiva riga e colonna.

Se volessimo calcolare la probabilità congiunta di trovare un uomo di prima classe $P(C=1,S=male)$ consideriamo il numero di uomini in prima classe diviso per il numero totale di passeggeri.

## Regola della somma (Probabilità Marginale)

La regola della somma e la regola del prodotto sono fondamentali e molto utilizzate. La regola della somma permette di calcolare probabilità univariate da probabilità congiunte e ci permette da fare da "ponte" è possibile passare da una congiunta ad una univariata in diversi modi. Le intersezioni della tabella $n{ij}$ è dato dal numero di volte in cui $X=x_i$ e $Y=y_i$.

La proabilità congiunta è dato dal numero di casi favorevoli $n{ij}$ fratto il numero di casi possibili $n$

La regola della somma ci dice che la probabilità che X=x è data dalla somma delle congiunte.

Ad esempio la probabilità di appartenere alla classe 1 P(C=1) è data dalla somma di P(C=1,S=m) + P(C=1, S=f)

## Probabilità Condizionale

La definiamo come la probabilità che avvenga un evento sapendo che un evento è già stato ottenuto, a differenza della probabilità congiunta i due eventi non si verificano contemporaneamente. Si definisce come il rapporto tra la probabilità congiunta P(X=x,Y=y) diviso per il numero di casi favorevoli P(Y=y)

In generale la probabilità di X dato Y è diversa dalla probabilità di X, stiamo considerando un caso più ristretto

## Regola del Prodotto (Fattorizzazione)

Divido la mia congiunta in due fattori, una condizionata ed una marginale.
Conviene perchè ci permette di calcolare la probabilità congiunta senza utilizzare tabelle grandi.

## Regola della Catena o Chain Rule Probability

Regola che ci permette di allenare reti neurali molto profonde tramite la tecnica di backpropagation. La probabilità congiunta è data dal prodotto della probabilità condizionata per la probabilità marginale. Questa regola è possibile applicarla anche quando abbiamo più variabili. Applicando in maniera ricorsiva la regola del prodotto ottengo la regola della catena.

P(A,B,C) = P(A | B,C) P(B|C) P(C)

## Teorema di Bayes

Il teorema è espresso come:

$$P\left( A \middle| B \right) = \frac{P\left( B \middle| A \right)P(A)}{P(B)}$$


Dove:
- P(A): Prende il nome di **probabilità a priori**
- P(B): Prende il nome di **evidenza**
- P(A|B): Prende il nome di **probabilità a posteriori**
- P(B|A): Prende il nome di **verosimiglianza**

Ci chiediamo come cambia la probabilità che avviene A se inglobo nuove informazioni.

## Esempio di probabilità Bayesiana

Vogliamo stimare la probabilità che un amico abbia il COVID-19.

Vogliamo stimare la probabilità che una persona abbia il COVID sapendo che ha la febbre.

La proabilità del COVID è una probabilità a priori, possiamo stimare che in questo momento presa una persona a caso abbia il COVID con una probabilità di 0.5
La probabilità che una persona presa a caso abbia la febbre possiami stimarla a 0.2. Verosimiglianza assumiamo che una persona su 3 abbia la febbre sapendo che abbia il COVID.

Applicando il teorema di Bayes

$$
P(COVID | Febbre) = \frac{0.3 * 0.5}{0.2} = 0.83
$$

## Indipendenza delle variabili

Due variabili sono indipendenti se una non influenza l'altra, peso ed altezza di una persona sono dipendenti tra loro, l'altezza di una persona e la sua ricchezza non sono variabili dipendenti.
Due variabili sono indipendenti se la probabilità della congiunta è uguale al prodotto delle marginali. O ancora la probabilità di x dato y è uguale alla probabilità di x.

## Indipendenza condizionale


Due variabili aleatorie X e Y si dicono **condizionalmente indipendenti** dato una variabile aleatoria Z se:

$$
P(X, Y \mid Z) = P(X \mid Z)  P(Y \mid Z)
$$

L’indipendenza condizionale può essere denotata come:

$$
X \perp Y \mid Z
$$

**Esempio**
L’altezza e il vocabolario non sono indipendenti: le persone più alte di solito sono anche più anziane e, di conseguenza, possiedono un vocabolario più ampio (conoscono più parole). Tuttavia, se si condiziona sull’età, le due variabili diventano indipendenti. Infatti, tra persone della stessa età, l’altezza non dovrebbe influenzare il vocabolario. Pertanto, altezza e vocabolario sono **condizionalmente indipendenti** rispetto all’età.

## Esempi di probabilità in Python

Calcolare la probabilità che un passeggero sia sopravvisuto?

```python
titanic['Survived'].value_counts(normalize=True)
```

Qual è la probabilità che un passeggero in prima classe sia sopravvisuto? **Probabilità congiunta**

```python
pd.crosstab(titanic['Survived'], titanic['Pclass'], normalize=True, margins=True)
```

Qual è la probabilità che un passeggero sia sopravvissuto, dato il fatto che era in terza classe? **Probabilità condizionata**

```python
pd.crosstab(titanic['Survived'], titanic['Pclass'], normalize=1, margins=True)
```