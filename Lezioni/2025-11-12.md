# Mercoledì 12 novembre 2025 - Classificazione e KNN

Nel caso della regressione abbiamo detto ched vogliamo costruire una funzione h definita in Omega con valori in R, nel caso di regressore multiplo è definita in R^n con valori in R.

Nei problemi di classificazione invece di predire un numero reale vogliamo predire una categoria, quindi lo spazio degli eventi di Y sarà discreto (categorico).

Definiamo un classificatore h definita in R con valori in {0,...,M-1}

$$h : \Re^n \to \{0,\ldots,M-1\}$$

Le applicazioni di esempio classiche sono:
- spam filter (sistema di classificazione binario {0,1})
- categorizzazione degli articoli dei social media in categorie come finanza, politica, sport...
- Riconoscimento di oggetti nelle immagini
- Diagnosi mediche e screening

Ho un training set di coppie con input ed output corretto, voglio minimizzare il rischio empirico utilizzando una Loss Function del tipo:

$$L(\hat y, y) = \begin{cases}1 &\text{ if }& \hat y \neq y \\ 0 &\text{ if }& \hat y=y \end{cases}$$

Il tasso di errore o Error Rate ovvero la percentuale di classificazioni errate è dato da:

$$R_{emp}(h) = \frac{1}{N} \sum_{i=1}^N L(\hat y_i, y_i) = \frac{\text{number of incorrect predictions}}{N}$$

Questa Loss ha problemi è una funzione definita a tratti, non è derivabile, vedremo funzioni di Loss migliori.

## Misure di Valutazione

Una delle misure più adottate è l'**accuracy** che prende come input gli insiemi di Y reali e predetti


$$Accuracy\left( Y_{TE},{\widehat{Y}}_{TE} \right) = \frac{\left| \left\{ y^{(i)} : y^{(i)} = {\widehat{y}}^{(i)} \right\} \right|}{|Y_{TE}|}$$

L'Error Rate diventa quindi il suo complementare:

$$ErrorRate\left( Y_{TE},{\widehat{Y}}_{TE} \right) = \frac{\left| \left\{ y^{(i)} : y^{(i)} \neq {\widehat{y}}^{(i)} \right\} \right|}{|Y_{TE}|} =  1- Accuracy\left( Y_{TE},{\widehat{Y}}_{TE} \right)$$

Ad esempio se un modello predice bene il 70% degli esempi la sua accuracy sarà di 0.7 e l'error rate associato sarà 0.3.

Queste misure di valuitazione hanno dei limiti quando i dataset sono fortemente sbilanciati, immaginiamo di avere 10000 elementi 500 in classe 0 e 9500 in classe 1. Un classificatore che predice sempre classe 1 (f(x)=1) avrà una accuracy del 95%

## Capire i tipi di errore

Nascono dai test statistici visti nelle scorse lezioni, abbiamo due tipi di errori:

- False Positive (Type 1): Classifico un campione negativo (etichetta 0) come un elemento positivo (etichetta 1) - False Alarm
- False Negative: Classifico un campione positivo (etichetta 1) come un elemento negativo (etichetta 0) - Miss

Abbiamo inoltre:
- True Positives: Elemento correttamente identificato di classe 1 - Hit
- True Negatives: Elemento correttamente identificati di classe 0

## La matrice di confusione

Per vedere a colpo d'occhio se il classifcatore funziona bene metto questi valori all'interno di una matrice.

metto sulle righe le etichette vere, la prima riga rappresenterà quello che è effettivamente positivo, la seconda riga quello che è effettivamente negativo, la prima colonna tutto quello che ho predetto come positivo, la seconda colonna tutto quello che ho predetto come negativo.

Idealmente vorrei avere valori alti nella diagonale e valori bassi fuori dalla diagonale.

Possiamo calcolare l'accuracy come: 

$$Accuracy = \frac{TP + TN}{TP + FN + FP + TN}$$

Ed è equivalente al rapporto tra positivi sul totale.

Nello spam detector meglio dare errori False Negative che False positive, meglio ricevere qualche email spam nella inbox che ricevere email non spam come spam.

## Dataset sbilanciato

Guardando la matrice di confusione notiamo un problema, una colonna è tutta 0 indica sbilanciamento dei dati

## Precision and Recall

Ci permettono di capire come è sbilanciato il mio classificatore. Entrambe misura una frazione dei True Positive

Sono definite come:

$$Precision = \frac{TP}{TP + FP}$$

nel denominatore abbiamo tutto quello che ho predetto come positivo (prima colonna)

$$Recall = \frac{TP}{TP + FN}$$

Nel denominatore abbiamo tutto che in realtà era positivo come ground-truth (prima riga)

In entrambe le formule non utilizziamo mai i True Negative.

La Recall misura tutto ciò che il mio modello si sta "perdendo", si pone la domanda: "Io ho 500 email di spam, tu quante ne hai trovate?". La precision mi permette di capire quanto è accurato il mio classificatore.

## Trade-Off tra Precision e Recall

Idealmente vorrei avere Precision e Recall alti, ma ci sono casi in cui il trade-off è ben definito, ad esempio gli spam detector è meglio che abbiano una precision alta. Supponiamo di avere un algoritmo di classificazione che mi permette di fare screening medico, in questo caso è meglio avere una Recall alta

## F1-Score: Bilanciare Precision e Recall

Un modo per aggregare Precision e Recall idelamente potremmo fare la media tra le due misure ma casi in cui ho precision 0 e recall 1 e casi in cui ho precision 0.5 e recall 0.5 avrebbero lo stesso risultato. Ma in realtà sono classificatori molto diversi. Risolviamo questo problema utilizzando la media arminica al posto della media aritmetica.

$$F_{1} = 2 \cdot \frac{precision \cdot recall}{precision + recall}$$

## Confusion Matrix Multi-Classe

La Matrice M ha l'elemento M[i,j] indica gli elementi di classe i classificati di classe j.

Le righe contengono gli elementi realmente classificati di classe i, le colonne gli elementi predetti in classe j.

## Curva ROC

La curva Receiver Operating Characteristic è utile per tutti quei classifixatori basati su soglia, ad esempio decido di classificare come positivo se il mio valore supera una certa soglia.

La curva ROC rappresenta sul piano dei punti che rappresentano dei trade-off ogni volta che scelgo una certa soglia. Introduciamo i True Positve Rate ed i False Positive Rate

$$TPR_\theta = \frac{TP_\theta}{TP_\theta+FN_\theta}$$

$$FPR_\theta = \frac{FP_\theta}{FP_\theta+TN_\theta}$$

idealmente vorrei una curva che abbia un basso false positive rate ed un true positive rate, quindi una curva molto vicina alla parte alta sinistra del grafico. Posso confrontare curve tra loro e capire quale tra le due tende a coprire meglio lo spazio ed arrivare al punto in alto a sinistra. 

L'Area Under The Curve è l'area sotto la curva, il cui massimo è 1, meglio avere un valore alto.

La bisettrice tratteggiata indica cosa otterrei in un caso random, mi aspetto man mano che sposto la mia soglia non cambierà nulla per ogni punto ottengo un perfetto bilanciamento tra TPR e FPR. Se trovo una curva sotto la bisettrice probabilmente bisogna solo invertire il segno dello score (o flippiamo le etichette) per ottenere una curva ribaltata.

Un predittore binario che sbaglia sempre è un predittore buono, basta cambiare segno!

## Classificatori basati su soglia

Ipotizziamo di voler classificare il genere di una persona sulla base della sua altezza, impostando una treshold a 170cm, mi accorgo che faccio più errori nel classificare le donne rispetto agli uomini. 

Guardando il **classification report** ci fornisce alcune misure di valutazione sul classificatore come:

- precision
- recall
- f1-score
- support (indica quanti elementi ho in una classe)

Le righe (False e True) indicano Male e Female (ma possiamo anche invertire le scelte)

Altre informazioni fornite sono:
- accuracy
- macro avg 
- weighted avg

Sono due modi diversi di calcolare l'accuracy per evitare errori di sbilanciamento, se un elemento A appare 3 volte su 10 ed un elemento B 7 volte su 10 lo peserò attribuendo il 70% del peso ad A ed il 30% del peso a B.

Spostando la soglia a 180 cm, ho una precision alta su male ed una recall bassa su male, una recall alta su male ed una precision bassa su female.

Guardando le metriche False indica Female e True indica Male, in effetti Recall di Female è 1 e Precision di Male è 1 ma i restanti due valori sono bassi. L'accuracy generale in tutti e 3 i calcoli è peggiorata.

Se calcolassi la curva ROC posso confrontare la curva ROC dell'altezza a quella relativa al Peso, guardando l'AUC ci fa capire che le altezze sono più discriminative dei pesi, ho un rapporto migliore segnale-rumore.

## Trovare la soglia giusta

Si trova solitamente tramite Cross Validation, un criterio potrebbe essere massimizzare l'accuracy o massimizzare l'F1-Score ma di solito si usa la J Statistic data da:

$$J = TPR + (1 - FPR)$$

Ci accorgiamo che la soglia ottimale è 172 cm non molto distante dal 170 ipotizzato.

Un classificatore con questa soglia fa aumentare l'accuracy e gli F1-Score rispetto a quello costruito con solgia 170.

## Nearest Neighbor Algorithm (1-NN)

Immaginiamo di avere un classificatore binario, il mio training set è formato da punti rossi e punti verdi, lo spazio ci suggerisce che tutto ciò che è relativo alla classe spam si trova sulla parte destra del grafico. 

Abbiamo bisogno di definire una misura di similarità, spesso si utiliza la distanza euclidea definita come:

$$d\left( \mathbf{x,}\mathbf{x}^{\mathbf{'}} \right) = \left\| \mathbf{x -}\mathbf{x}^{\mathbf{'}} \right\|_{2} = \sqrt{\sum_{i = 1}^{N}\left( x_{i} - x_{i}^{'} \right)^{2}}$$

Vogliamo minimizzare 

$$h\left( \mathbf{x}^{\mathbf{'}} \right) = \arg_{y}{\min{\{ d\left( \mathbf{x}^{\mathbf{'}}\mathbf{,\ x} \right)|\left( \mathbf{x},y \right) \in TR\}}}$$

## K Nearest Neighbor

Definiamo il vicinato come:

$$N_K(\mathbf{x'}) = N(\mathbf{x'},R_K(\mathbf{x'}))$$

dove k è un iperparametro del mio algoritmo, il migliore lo si trova mediante cross validation.

Definiamo KNN come:

$$h\left( \mathbf{x'} \right) = mode\{ y|\left( \mathbf{x},y \right) \in N\left( \mathbf{x'};TR,\ K \right)\}$$

## KNN's Greatest Weakness

Man mano che aumento il numero di variabili la complessità aumenta quadraticamente e non linearmente.

Inoltre con più dimensioni le distanze diventano sempre più sparse, le distanze minime e massime aumentano in maniera simile il loro rapporto si comporta in maniera diversa. All'inizio questo numero è molto grande man mao che i punti diventano più sparsi il rapporto tra le distanze collassa. Questo prende il nome di collasso del vicinato.

## KNN: Un classificatore discriminativo

Una distinzione che facciamo è quella tra classificatori discriminativi e classificatori generativi

Un classificatore discriminativo cerca di distringuere i valori tra di loro (come quelli basati su soglia), il classificatore generativo cerca di capire qual è il processo che ha generato i dati.

I metodi generativi fanno tante assunzioni sui dati ed hanno bisogno di meno dati per funzionare, quelli discriminativi fanno meno assunzioni ed hanno bisogno di più dati.

I generativi cercano di modellare la probabilità congiunta ed utilizzano il teorema di Bayes stimando P(X|Y).

Il classificatore discriminativo cerca di trovare una funzione che mi dice se è più probabile essere in classe 1 o in classe 2. Il KNN stima la probabilità P(y = c | x) come il numero di elementi x appartenenti al vicinato la cui classe è c diviso k. 

## KNN Nel Dataset Iris

Abbiamo 3 specie di iris setosa, versicolor e virginica. Le caratteristiche che mi permettono di classificare questi fiori sono lunghezza ed altezza del petalo ed il sepalo. Inoltre abbiamo la specie che indica la nostra ground-truth.

Se andassi a fare uno scattperlot tra lunghezza del petalo e larghezza del petalo.

Prendo una griglia regolare, li passo alla mia funzione di classificazione h e vedo che output gli viene associato e faccio un grafico chiamato **classification map**. Il decision boundary è il punto in cui jho massima incertezza nel classificare.

In generale un singolo punto cambia il decision boundary, con k=1 ho alta varianza, se aumento k il mio decision boundary è più smooth ha un bias più alto ed è più flessibile ed ho diminuito la varianza, rappresenta un classificatore migliore. Con k=n che è il valore più alto che io posso immaginare restituisce un solo valore, la classe più frequente nel dataset. Ho un bias enorme ed una varianza bassissima.

## Come trovare K tramite Cross Validation

Sul training set troverò sempre k=1, divido il mio dataset in train/test/validation e cerco k proprio in questi ultimi due. Prendo il valore di k che ha media più alta.

## Regressione tramite KNN

KNN può essere usato anche come regressore ed è non parametrico, questo algoritmo fa trasduzione trasforma un elemento in un altro elemento guardando una banca dati di riferimento. Stessa cosa che succede nei RAG Graphs.

Anche nel caso della regressione trovo il k migliore tramite cross validation che minimizza l'MSE.